# -*- coding: utf-8 -*-
"""self_att_selective_att.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k1qc54j59Qfrf1aTXbGxve8Qqtor3HyL
"""

# imports for model and trainer
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import os
import datetime
import sys
import logging
import sklearn.metrics
import math
from tqdm import tqdm

def to_var(x):
    return torch.from_numpy(x).cuda()

class Accuracy(object):
    def __init__(self):
        self.correct = 0
        self.total = 0

    def add(self, is_correct):
        self.total += 1
        if is_correct:
            self.correct += 1

    def get(self):
        if self.total == 0:
            return 0.0
        else:
            return float(self.correct) / self.total

    def clear(self):
        self.correct = 0
        self.total = 0


class Config():
    def __init__(self):
        self.acc_NA = Accuracy()
        self.acc_not_NA = Accuracy()
        self.acc_total = Accuracy()
        self.data_path = "/content/drive/My Drive/Colab Notebooks/BS/data"
        self.log_dir = "/content/drive/My Drive/Colab Notebooks/BS/logs"
        self.use_bag = True
        self.use_gpu = True
        self.is_training = True
        self.max_length = 120
        self.pos_num = 2 * self.max_length
        self.num_classes = 53
        self.hidden_size = 230  # CNN filter size
        self.max_epoch = 15
        self.opt_method = 'SGD'
        self.optimizer = None
        self.learning_rate = 0.5
        self.weight_decay = 1e-5
        self.drop_prob = 0.5
        self.checkpoint_dir = '/content/drive/My Drive/Colab Notebooks/BS/checkpoint'
        self.test_result_dir = '/content/drive/My Drive/Colab Notebooks/BS/test_result'
        self.save_epoch = 1
        self.test_epoch = 1
        self.pretrain_model = None
        self.trainModel = None
        self.testModel = None
        self.batch_size = 160
        self.word_size = 50
        self.pos_size = 5
        self.window_size = 3
        self.epoch_range = None
        self.input_dim = self.word_size + 2 * self.pos_size
        self.att_n_blocks = 1
        self.att_n_head = 6
        self.att_d_ff = 256
        self.att_d_model = self.input_dim
        self.att_d_inner = 115
        self.att_d_output = self.hidden_size * 3
        self.att_dropout = 0.1
        self.start_epoch = 0

    def init_logger(self, log_name):
        if not os.path.exists(self.log_dir):
            os.mkdir(self.log_dir)
        logger = logging.getLogger(__name__)
        logger.setLevel(level=logging.DEBUG)
        log_file_name = log_name + ".log"
        log_handler = logging.FileHandler(os.path.join(self.log_dir, log_file_name), "w")
        log_format = logging.Formatter("%(asctime)s: %(message)s")
        log_handler.setFormatter(log_format)
        logger.addHandler(log_handler)
        self.logger = logger

    def load_train_data(self):
        print("Reading training data...")
        self.data_word_vec = np.load(os.path.join(self.data_path, 'vec.npy'))
        self.data_train_word = np.load(os.path.join(self.data_path, 'train_word.npy'))
        self.data_train_pos0 = np.load(os.path.join(self.data_path, 'train_pos0.npy'))

        self.data_train_pos1 = np.load(os.path.join(self.data_path, 'train_pos1.npy'))
        self.data_train_pos2 = np.load(os.path.join(self.data_path, 'train_pos2.npy'))
        self.data_train_mask = np.load(os.path.join(self.data_path, 'train_mask.npy'))
        if self.use_bag:
            self.data_query_label = np.load(os.path.join(self.data_path, 'train_ins_label.npy'))
            self.data_train_label = np.load(os.path.join(self.data_path, 'train_bag_label.npy'))
            self.data_train_scope = np.load(os.path.join(self.data_path, 'train_bag_scope.npy'))
        else:
            self.data_train_label = np.load(os.path.join(self.data_path, 'train_ins_label.npy'))
            self.data_train_scope = np.load(os.path.join(self.data_path, 'train_ins_scope.npy'))
        print("Finish reading")
        self.train_order = list(range(len(self.data_train_label)))
        self.train_batches = len(self.data_train_label) // self.batch_size
        if len(self.data_train_label) % self.batch_size != 0:
            self.train_batches += 1

    def load_test_data(self):
        print("Reading testing data...")
        self.data_word_vec = np.load(os.path.join(self.data_path, 'vec.npy'))
        self.data_test_word = np.load(os.path.join(self.data_path, 'test_word.npy'))
        self.data_test_pos0 = np.load(os.path.join(self.data_path, 'test_pos0.npy'))

        self.data_test_pos1 = np.load(os.path.join(self.data_path, 'test_pos1.npy'))
        self.data_test_pos2 = np.load(os.path.join(self.data_path, 'test_pos2.npy'))
        self.data_test_mask = np.load(os.path.join(self.data_path, 'test_mask.npy'))
        if self.use_bag:
            self.data_test_label = np.load(os.path.join(self.data_path, 'test_bag_label.npy'))
            self.data_test_scope = np.load(os.path.join(self.data_path, 'test_bag_scope.npy'))
        else:
            self.data_test_label = np.load(os.path.join(self.data_path, 'test_ins_label.npy'))
            self.data_test_scope = np.load(os.path.join(self.data_path, 'test_ins_scope.npy'))
        print("Finish reading")
        self.test_batches = len(self.data_test_label) // self.batch_size
        if len(self.data_test_label) % self.batch_size != 0:
            self.test_batches += 1

        self.total_recall = self.data_test_label[:, 1:].sum()

    def set_train_model(self, model):
        print("Initializing training model...")
        self.model = model
        self.trainModel = self.model(config=self)
        if self.pretrain_model is not None:
            self.trainModel.load_state_dict(torch.load(self.pretrain_model))
        self.trainModel.cuda()
        if self.optimizer is not None:
            pass
        elif self.opt_method == "Adagrad" or self.opt_method == "adagrad":
            self.optimizer = optim.Adagrad(self.trainModel.parameters(), lr=self.learning_rate, lr_decay=self.lr_decay,
                                           weight_decay=self.weight_decay)
        elif self.opt_method == "Adadelta" or self.opt_method == "adadelta":
            self.optimizer = optim.Adadelta(self.trainModel.parameters(), lr=self.learning_rate,
                                            weight_decay=self.weight_decay)
        elif self.opt_method == "Adam" or self.opt_method == "adam":
            self.optimizer = optim.Adam(self.trainModel.parameters(), lr=self.learning_rate,
                                        weight_decay=self.weight_decay)
        else:
            self.optimizer = optim.SGD(self.trainModel.parameters(), lr=self.learning_rate,
                                       weight_decay=self.weight_decay)
        print("Finish initializing")

    def set_test_model(self, model):
        print("Initializing test model...")
        self.model = model
        self.testModel = self.model(config=self)
        self.testModel.cuda()
        self.testModel.eval()
        print("Finish initializing")
    
    def adjust_learning_rate(self, epoch):
        """Sets the learning rate to the initial LR decayed by 10 every 10 epochs"""
        lr = self.learning_rate * (0.1 ** (epoch // 5))
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

    def get_train_batch(self, batch):
        input_scope = np.take(self.data_train_scope,
                              self.train_order[batch * self.batch_size: (batch + 1) * self.batch_size], axis=0)
        index = []
        scope = [0]
        for num in input_scope:
            index = index + list(range(num[0], num[1] + 1))
            scope.append(scope[len(scope) - 1] + num[1] - num[0] + 1)
        self.batch_word = self.data_train_word[index, :]
        self.batch_pos0 = self.data_train_pos0[index, :]
        self.batch_pos1 = self.data_train_pos1[index, :]
        self.batch_pos2 = self.data_train_pos2[index, :]
        self.batch_mask = self.data_train_mask[index, :]
        self.batch_label = np.take(self.data_train_label,
                                   self.train_order[batch * self.batch_size: (batch + 1) * self.batch_size], axis=0)
        self.batch_attention_query = self.data_query_label[index]
        self.batch_scope = scope

    def get_test_batch(self, batch):
        input_scope = self.data_test_scope[batch * self.batch_size: (batch + 1) * self.batch_size]
        index = []
        scope = [0]
        for num in input_scope:
            index = index + list(range(num[0], num[1] + 1))
            scope.append(scope[len(scope) - 1] + num[1] - num[0] + 1)
        self.batch_word = self.data_test_word[index, :]
        self.batch_pos0 = self.data_test_pos0[index, :]

        self.batch_pos1 = self.data_test_pos1[index, :]
        self.batch_pos2 = self.data_test_pos2[index, :]
        self.batch_mask = self.data_test_mask[index, :]
        self.batch_scope = scope

    def train_one_step(self):
        self.trainModel.embedding.word = to_var(self.batch_word)
        self.trainModel.embedding.pos0 = to_var(self.batch_pos0)
        self.trainModel.embedding.pos1 = to_var(self.batch_pos1)
        self.trainModel.embedding.pos2 = to_var(self.batch_pos2)
        self.trainModel.encoder.mask = to_var(self.batch_mask)
        self.trainModel.selector.scope = self.batch_scope
        self.trainModel.selector.attention_query = to_var(self.batch_attention_query)
        self.trainModel.selector.label = to_var(self.batch_label)
        self.trainModel.classifier.label = to_var(self.batch_label)
        self.optimizer.zero_grad()
        loss, _output = self.trainModel()
        loss.backward()
        self.optimizer.step()
        # print("prediction: ", _output.tolist())
        # print("gt label: ", self.batch_label.tolist())
        for i, prediction in enumerate(_output):
            if self.batch_label[i] == 0:
                self.acc_NA.add(prediction == self.batch_label[i])
            else:
                self.acc_not_NA.add(prediction == self.batch_label[i])
            self.acc_total.add(prediction == self.batch_label[i])
        return loss.item()

    def test_one_step(self):
        self.testModel.embedding.word = to_var(self.batch_word)
        self.testModel.embedding.pos0 = to_var(self.batch_pos0)

        self.testModel.embedding.pos1 = to_var(self.batch_pos1)
        self.testModel.embedding.pos2 = to_var(self.batch_pos2)
        self.testModel.encoder.mask = to_var(self.batch_mask)
        self.testModel.selector.scope = self.batch_scope
        return self.testModel.test()

    def train(self):
        if not os.path.exists(self.checkpoint_dir):
            os.mkdir(self.checkpoint_dir)
        best_auc = 0.0
        best_p = None
        best_r = None
        best_epoch = 0
        self.init_logger("train-" + self.model.__name__)
        for epoch in range(self.start_epoch, self.max_epoch):
            print('Epoch ' + str(epoch) + ' starts...')
            self.logger.info('Epoch ' + str(epoch) + ' starts...')
            self.acc_NA.clear()
            self.acc_not_NA.clear()
            self.acc_total.clear()
            self.adjust_learning_rate(epoch + 1)
            np.random.shuffle(self.train_order)
            for batch in range(self.train_batches):
                self.get_train_batch(batch)
                loss = self.train_one_step()
                time_str = datetime.datetime.now().isoformat()
                print(
                    "epoch %d step %d time %s | loss: %f, NA accuracy: %f, not NA accuracy: %f, total accuracy: %f\r" % (
                        epoch, batch, time_str, loss, self.acc_NA.get(), self.acc_not_NA.get(), self.acc_total.get()))
                self.logger.info(
                    "epoch %d step %d time %s | loss: %f, NA accuracy: %f, not NA accuracy: %f, total accuracy: %f\r" % (
                        epoch, batch, time_str, loss, self.acc_NA.get(), self.acc_not_NA.get(), self.acc_total.get()))
            if (epoch + 1) % self.save_epoch == 0:
                print('Epoch ' + str(epoch) + ' has finished')

                self.testModel = self.trainModel
                auc, pr_x, pr_y = self.test_one_epoch()
                # np.save(os.path.join(self.test_result_dir, self.model.__name__ + "{}-{}".format(epoch, auc) + '_x.npy'),
                #         pr_x)
                # np.save(os.path.join(self.test_result_dir, self.model.__name__ + "{}-{}".format(epoch, auc) + '_y.npy'),
                #         pr_y)
                print('Saving model...')
                self.logger.info('Epoch ' + str(epoch) + ' has finished')
                self.logger.info('Saving model...')
                path = os.path.join(self.checkpoint_dir, self.model.__name__ + '-' + str(epoch) + "-" + str(auc))
                torch.save(self.trainModel.state_dict(), path)
                print('Have saved model to ' + path)
                self.logger.info('Have saved model to ' + path)

                if auc > best_auc:
                    best_auc = auc
                    best_p = pr_x
                    best_r = pr_y
                    best_epoch = epoch

            # if (epoch + 1) % self.test_epoch == 0:

        print("Finish training")
        print("Best epoch = %d | auc = %f" % (best_epoch, best_auc))
        print("Storing best result...")
        self.logger.info("Finish training")
        self.logger.info("Best epoch = %d | auc = %f" % (best_epoch, best_auc))
        self.logger.info("Storing the best result...")

        if not os.path.isdir(self.test_result_dir):
            os.mkdir(self.test_result_dir)
        np.save(os.path.join(self.test_result_dir, self.model.__name__ + '_x.npy'), best_p)
        np.save(os.path.join(self.test_result_dir, self.model.__name__ + '_y.npy'), best_r)
        print("Finish storing")
        self.logger.info("Finish storing")

    def test_one_epoch(self):
        test_score = []
        for batch in tqdm(range(self.test_batches)):
            self.get_test_batch(batch)
            batch_score = self.test_one_step()
            test_score = test_score + batch_score
        test_result = []
        for i in range(len(test_score)):
            for j in range(1, len(test_score[i])):
                test_result.append([self.data_test_label[i][j], test_score[i][j]])
        test_result = sorted(test_result, key=lambda x: x[1])
        test_result = test_result[::-1]
        pr_x = []
        pr_y = []
        correct = 0
        for i, item in enumerate(test_result):
            correct += item[0]
            pr_y.append(float(correct) / (i + 1))
            pr_x.append(float(correct) / self.total_recall)
        auc = sklearn.metrics.auc(x=pr_x, y=pr_y)
        print("auc: ", auc)
        return auc, pr_x, pr_y

    def test(self):
        best_epoch = None
        best_auc = 0.0
        best_p = None
        best_r = None
        self.init_logger("test-" + self.model.__name__)
        for epoch in self.epoch_range:
            path = os.path.join(self.checkpoint_dir, self.model.__name__ + '-' + str(epoch))
            if not os.path.exists(path):
                continue
            print("Start testing epoch %d" % (epoch))
            self.logger.info("Start testing epoch %d" % (epoch))
            self.testModel.load_state_dict(torch.load(path))
            auc, p, r = self.test_one_epoch()
            if auc > best_auc:
                best_auc = auc
                best_epoch = epoch
                best_p = p
                best_r = r
            print("Finish testing epoch %d" % (epoch))
            self.logger.info("Finish testing epoch %d" % (epoch))

        print("Best epoch = %d | auc = %f" % (best_epoch, best_auc))
        self.logger.info("Best epoch = %d | auc = %f" % (best_epoch, best_auc))
        print("Storing best result...")
        self.logger.info("Storing best result...")
        if not os.path.isdir(self.test_result_dir):
            os.mkdir(self.test_result_dir)
        np.save(os.path.join(self.test_result_dir, self.model.__name__ + '_x.npy'), best_p)
        np.save(os.path.join(self.test_result_dir, self.model.__name__ + '_y.npy'), best_r)
        print("Finish storing")
        self.logger.info("Finish storing")

class Embedding(nn.Module):
    def __init__(self, config):
        super(Embedding, self).__init__()
        self.config = config
        self.word_embedding = nn.Embedding(self.config.data_word_vec.shape[0], self.config.data_word_vec.shape[1])
        self.pos0_embedding = nn.Embedding(self.config.pos_num, self.config.pos_size, padding_idx = 0)
        self.pos1_embedding = nn.Embedding(self.config.pos_num, self.config.pos_size, padding_idx = 0)
        self.pos2_embedding = nn.Embedding(self.config.pos_num, self.config.pos_size, padding_idx = 0)
        self.init_word_weights()
        self.init_pos_weights()
        self.word = None
        self.pos0 = None
        self.pos1 = None
        self.pos2 = None

    def init_word_weights(self):
        self.word_embedding.weight.data.copy_(torch.from_numpy(self.config.data_word_vec))

    def init_pos_weights(self):
        nn.init.xavier_uniform_(self.pos0_embedding.weight.data)
        if self.pos0_embedding.padding_idx is not None:
            self.pos0_embedding.weight.data[self.pos0_embedding.padding_idx].fill_(0)
        nn.init.xavier_uniform_(self.pos1_embedding.weight.data)
        if self.pos1_embedding.padding_idx is not None:
            self.pos1_embedding.weight.data[self.pos1_embedding.padding_idx].fill_(0)
        nn.init.xavier_uniform_(self.pos2_embedding.weight.data)
        if self.pos2_embedding.padding_idx is not None:
            self.pos2_embedding.weight.data[self.pos2_embedding.padding_idx].fill_(0)

    def forward(self):
        word = self.word_embedding(self.word)
        # pos0 = self.pos0_embedding(self.pos0)
        pos1 = self.pos1_embedding(self.pos1)
        pos2 = self.pos2_embedding(self.pos2)
        embedding = torch.cat((word, pos1, pos2), dim=2)
        return embedding

# Attention Layer
class PositionalEmbedding(nn.Module):
    def __init__(self, d_w2v, d_pos, max_len=512):
        super().__init__()   
        # Compute the positional encodings once in log space.
        pe = torch.zeros(max_len, d_w2v)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_w2v, 2).float() *
                             -(math.log(10000.0) / d_w2v))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        padding = torch.zeros(max_len, d_pos * 2)
        pe = torch.cat([pe, padding], dim=1)
        pe = pe.unsqueeze(0)
        self.weight = nn.Parameter(pe, requires_grad=False)
         
    def forward(self, x):
        return x + self.weight[:, :x.size(1), :]


class SelfAttEncoder(nn.Module):
    """
    A encoder model with self attention mechanism.
    """
    def __init__(self, config):
        super().__init__()
        # self.pos_encoder = PositionalEmbedding(config.word_size, config.pos_size, config.max_length)
        self.attn_head = MultiHeadAttention(config.att_d_model, config.att_d_inner, config.att_n_head, config.att_dropout)
        self.layer_norm1 = nn.LayerNorm(config.att_d_model)
        self.dropout = nn.Dropout(config.att_dropout)
        self.position_wise_feed_forward = nn.Sequential(
            nn.Linear(config.att_d_model, config.att_d_ff),
            nn.ReLU(),
            nn.Linear(config.att_d_ff, config.att_d_model),
        )
        self.layer_norm2 = nn.LayerNorm(config.att_d_model)
        self.output_fc1 = nn.Linear(config.att_d_model, config.hidden_size)
        self.output_fc2 = nn.Linear(config.hidden_size, config.hidden_size * 3)
         
    def forward(self, x):
        # x = self.pos_encoder(x)
        att = self.attn_head(x, x, x)
        x = x + self.dropout(self.layer_norm1(att))
        # pos = /self.position_wise_feed_forward(x)
        x = x + self.dropout(self.layer_norm2(x))
        x = self.output_fc1(x)
        x = self.output_fc2(x)
        return torch.max(x, dim=1)[0]
        # return x


class MultiHeadAttention(nn.Module):
    """The full multihead attention block"""
    def __init__(self, d_model, d_inner, n_head, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.d_inner = d_inner
        self.n_head = n_head
 
        self.attn_head = nn.ModuleList([
            AttentionHead(d_model, d_inner, dropout) for _ in range(n_head)
        ])
        self.projection = nn.Linear(d_inner * n_head, d_model) 
     
    def forward(self, queries, keys, values):
        x = [attn(queries, keys, values) # (Batch, Seq, Feature)
             for i, attn in enumerate(self.attn_head)]
         
        x = torch.cat(x, 2) # (Batch, Seq, d_inner * n_head)
        x = self.projection(x) # (Batch, Seq, D_Model)
        return x


class AttentionHead(nn.Module):
    """A single attention head"""
    def __init__(self, d_model, d_inner, dropout=0.1):
        super().__init__()
        # We will assume the queries, keys, and values all have the same feature size
        self.attn = ScaledDotProductAttention(dropout)
        self.query_tfm = nn.Linear(d_model, d_inner)
        self.key_tfm = nn.Linear(d_model, d_inner)
        self.value_tfm = nn.Linear(d_model, d_inner)
 
    def forward(self, queries, keys, values):
        Q = self.query_tfm(queries) # (Batch, Seq, Feature)
        K = self.key_tfm(keys) # (Batch, Seq, Feature)
        V = self.value_tfm(values) # (Batch, Seq, Feature)
        x = self.attn(Q, K, V)
        return x

class ScaledDotProductAttention(nn.Module):
    """
    Scaled Dot-Product Attention
    """

    def __init__(self, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
 
    def forward(self, q, k, v):
        d_k = k.size(-1) # get the size of the key 
        attn = F.softmax(torch.bmm(q / d_k**0.5, k.transpose(1, 2)), dim = 2)
        attn = self.dropout(attn)
        output = torch.bmm(attn, v) 
        return output

# PCNN block
class PCNN(nn.Module):
    def __init__(self, config):
        super(PCNN, self).__init__()
        self.config = config
        self.mask = None
        self.cnn = _CNN(config)
        self.pooling = _PiecewisePooling()
        self.activation = nn.ReLU()

    def forward(self, embedding):
        embedding = torch.unsqueeze(embedding, dim=1)
        x = self.cnn(embedding)
        x = self.pooling(x, self.mask, self.config.hidden_size)
        return self.activation(x)


class _CNN(nn.Module):
    def __init__(self, config):
        super(_CNN, self).__init__()
        self.config = config
        self.in_channels = 1
        self.in_height = self.config.max_length
        self.in_width = self.config.word_size + 2 * self.config.pos_size
        self.kernel_size = (self.config.window_size, self.in_width)
        self.out_channels = self.config.hidden_size
        self.stride = (1, 1)
        self.padding = (1, 0)
        self.cnn = nn.Conv2d(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.padding)

    def forward(self, embedding):
        return self.cnn(embedding)


class _PiecewisePooling(nn.Module):
    def __init(self):
        super(_PiecewisePooling, self).__init__()

    def forward(self, x, mask, hidden_size):
        mask = torch.unsqueeze(mask, 1)
        x, _ = torch.max(mask + x, dim=2)
        x = x - 100
        return x.view(-1, hidden_size * 3)

# selector Layer
class Attention(nn.Module):
    """
    self attentive slector as mentioned in paper Lin
    """
    def __init__(self, config, relation_dim):
        super(Attention, self).__init__()
        self.config = config
        self.relation_matrix = nn.Embedding(self.config.num_classes, relation_dim)  # 53, d
        self.bias = nn.Parameter(torch.Tensor(self.config.num_classes))
        self.attention_matrix = nn.Embedding(self.config.num_classes, relation_dim)
        self.init_weights()
        self.scope = None
        self.attention_query = None
        self.label = None
        self.dropout = nn.Dropout(self.config.drop_prob)

    def init_weights(self):
        nn.init.xavier_uniform_(self.relation_matrix.weight.data)
        nn.init.normal_(self.bias)
        nn.init.xavier_uniform_(self.attention_matrix.weight.data)

    def get_logits(self, x):
        logits = torch.matmul(x, torch.transpose(self.relation_matrix.weight, 0, 1), ) + self.bias
        return logits

    def _attention_train_logit(self, x):
        relation_query = self.relation_matrix(self.attention_query)
        attention = self.attention_matrix(self.attention_query)
        attention_logit = torch.sum(x * attention * relation_query, 1, True)
        return attention_logit

    def _attention_test_logit(self, x):
        attention_logit = torch.matmul(x, torch.transpose(self.attention_matrix.weight * self.relation_matrix.weight, 0,
                                                          1))
        return attention_logit

    def forward(self, x):
        attention_logit = self._attention_train_logit(x)  # n, 53
        tower_repre = []
        for i in range(len(self.scope) - 1):
            sen_matrix = x[self.scope[i]: self.scope[i + 1]]
            attention_score = F.softmax(torch.transpose(attention_logit[self.scope[i]: self.scope[i + 1]], 0, 1), 1)
            final_repre = torch.squeeze(torch.matmul(attention_score, sen_matrix))
            tower_repre.append(final_repre)
        stack_repre = torch.stack(tower_repre)
        stack_repre = self.dropout(stack_repre)
        logits = self.get_logits(stack_repre)
        return logits

    def test(self, x):
        attention_logit = self._attention_test_logit(x)  # n, 53
        tower_output = []
        for i in range(len(self.scope) - 1):
            sen_matrix = x[self.scope[i]: self.scope[i + 1]]  # b, d
            #  53, b
            attention_score = F.softmax(torch.transpose(attention_logit[self.scope[i]: self.scope[i + 1]], 0, 1), 1)
            final_repre = torch.matmul(attention_score, sen_matrix)  # 53, d
            logits = self.get_logits(final_repre)  # 53, 53
            tower_output.append(torch.diag(F.softmax(logits, 1)))  # 53
        stack_output = torch.stack(tower_output)
        return list(stack_output.data.cpu().numpy())


class relation_toward_attention(Attention):
  """
  attention machanism toward relation to redestribute bag level weights
  we will rewrite train and test func here
  """
  def forward(self, x):
      attention_logit = self._attention_train_logit(x)  # n, 53
      tower_repre = []
      for i in range(len(self.scope) - 1):
          sen_matrix = x[self.scope[i]: self.scope[i + 1]]
          attention_score = F.softmax(torch.transpose(attention_logit[self.scope[i]: self.scope[i + 1]], 0, 1), 1)
          final_repre = torch.squeeze(torch.matmul(attention_score, sen_matrix))
          tower_repre.append(final_repre)
      stack_repre = torch.stack(tower_repre)
      stack_repre = self.dropout(stack_repre)
      logits = self.get_logits(stack_repre)
      return logits

  def test(self, x):
      attention_logit = self._attention_test_logit(x)  # n, 53
      tower_output = []
      for i in range(len(self.scope) - 1):
          sen_matrix = x[self.scope[i]: self.scope[i + 1]]  # b, d
          #  53, b
          attention_score = F.softmax(torch.transpose(attention_logit[self.scope[i]: self.scope[i + 1]], 0, 1), 1)
          final_repre = torch.matmul(attention_score, sen_matrix)  # 53, d
          logits = self.get_logits(final_repre)  # 53, 53
          tower_output.append(torch.diag(F.softmax(logits, 1)))  # 53
      stack_output = torch.stack(tower_output)
      return list(stack_output.data.cpu().numpy())

# classifiers
class Classifier(nn.Module):
    def __init__(self, config):
        super(Classifier, self).__init__()
        self.config = config
        self.label = None
        self.loss = nn.CrossEntropyLoss()

    def forward(self, logits):
        loss = self.loss(logits, self.label)
        _, output = torch.max(logits, dim=1)
        return loss, output.data

# model defination
class self_att_selective_att(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.encoder = SelfAttEncoder(config) 
        # self.encoder = PCNN(config)
        self.selector = Attention(config,  config.hidden_size*3)
        self.embedding = Embedding(config)
        self.classifier = Classifier(config)
        self.mask = None

    def forward(self):
        embedding = self.embedding()
        # sen_vec = self.encoder0(embedding)
        sen_vec = self.encoder(embedding)
        logits = self.selector(sen_vec)
        return self.classifier(logits)

    def test(self):
        embedding = self.embedding()
        # sen_vec = self.encoder0(embedding)   
        sen_vec = self.encoder(embedding)     
        return self.selector.test(sen_vec)

def train_model():
    from google.colab import drive
    drive.mount('/content/drive')

    con = Config()
    # con.data_path = "/content/drive/My Drive/Colab Notebooks/BS/mini_data" 
    con.batch_size = 80
    con.opt_method = "adam"
    con.learning_rate = 0.001 
    con.weight_decay = 1e-4
    con.max_epoch = 60
    con.load_train_data()   
    con.load_test_data()
    con.att_dropout = 0.5
    con.start_epoch = 23
    con.pretrain_model = "/content/drive/My Drive/Colab Notebooks/BS/checkpoint/self_att_selective_att-22-0.2944298884997939"
    con.set_train_model(self_att_selective_att)
    con.train()


# train_model()
con = Config()
con.data_path = r"D:\Code\MyCodes\BS\mini_data"
con.load_train_data()    
pretrained_model = self_att_selective_att(con)

pretrained_model.load_state_dict(torch.load(r"D:\Code\MyCodes\BS\self_att_selective_att-20-0.3018619105308484", map_location="cpu"))
update_dict = {k:v for k, v in pretrained_model.state_dict().items() if not k.startswith(("selector", "classifier", "embedding.word"))}
# print(update_dict.keys())
from self_att_cnn_selective_att import self_att_cnn_selective_att
model = self_att_cnn_selective_att(con)
model_dict = model.state_dict()
# # for p in model.parameters():
# #     print(p)
# update_dict = {k: v for k,v in pretrained_model.state_dict().items() if k not in model.state_dict().keys() and not k.startsWith("")}
# print(update_dict.values())
freeze_id = []
model_dict.update(update_dict)
model.load_state_dict(model_dict)
for i, p in enumerate(model_dict):
    if not p in update_dict:
        # freeze_id.append(i)
        print(i, p)
print(freeze_id)
# for i,p in enumerate(model.parameters()):
#     if i in freeze_id:
#         p.requires_grad = False
# # print(model.state_dict().keys())
# torch.save(model.state_dict(), r"D:\Code\MyCodes\BS\self_att_cnn_selective_att")

