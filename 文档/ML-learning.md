# 统计学习方法学习笔记
[TOC]
## 监督学习
本书主要讨论监督学习，这种情况下我们使用统计学习的方法可以概括如下：

    1. 从给定的、有限的、用于学习的训练数据集合出发，假设数据是独立同分布产生的；
    2. 假设要学习的模型属于某个函数的集合，称为假设空间；
    3. 应用某个评价标准，从假设空间中选取一个最优的模型；
    4. 最优模型的选取由算法实现。
统计学习方法包括模型的假设空间，模型选择的准则以及模型模型学习的算法，称为统计学习方法的三要素


## 统计学习常用的损失函数有以下几种
1. 0-1损失函数
    $$
    L(Y,f(X)) =
    \begin{cases}
    1,  & Y\ne f(X) \\
    0,  & Y=f(X)
    \end{cases}
    $$
2. 平方损失函数
    $$ L(Y,f(X)) =(Y-f(X))^2$$
3. 绝对损失函数
    $$ L(Y,f(X)) =|Y-f(X)|$$
4. 对数损失函数(信息量)
    $$ L(Y,f(X)) =-\log P(Y|X)$$

由于模型的输入、输出$(X,Y)$是随机变量，遵循联合分布$P(X,Y)$，所以损失函数也遵循$P(X,Y)$分布，损失函数的期望是
$$R_{exp(f)}=E_P[L(Y,f(X))]=\int_{x*y} {L(y,f(x))P(x,y)} {\rm dxdy}$$

这是理论上模型$f(X)$关于联合分布$P(X,Y)$平均意义下的损失，称为**风险函数**或者**期望损失**。

**学习的目的就是选择期望风险最小的模型。**

上文说到的期望风险是模型关于联合分布的期望损失，实际中我们往往只能获取训练数据集的平均损失，称为经验风险$R_{emp(f)}$。
根据大数定律，当样本容量区域无穷时，经验风险$R_{emp(f)}$趋于期望风险$R_{exp(f)}$。但是实际中往往又很难获取到真正足够量的数据，所以经验风险与期望风险有偏差，我们只能对经验风险进行一定的矫正：

## 经验风险最小化与结构风险最小化
经验风险最小化认为，经验风险最小的模型就是最优的模型，就是说只学习训练集就够了。
当训练集足够大的时候，经验风险最小化能保证有很好的学习效果，在现实中被广泛采用。
但是当样本容量很小的时候，经验风险最小化学习的效果就未必很好，会产生过拟合现象。

结构风险最小化是为了防止过拟合而提出来的策略。**结构风险最小化等价于正则化**.
结构风险最小化的形式往往是在经验风险上加上表示模型复杂度的正则化项或者惩罚项。
结构风险的定义是
$$\min_{f \in F}\frac{1}{N}\sum_{i=1}^N L(y_i,f(x_i))+\lambda J(f)$$


## 正则化
正则化项可以取不同的形式，例如，回归问题中，损失函数是平方损失，正则化项可以是参数向量的L2范数
$$L(w)=\frac{1}{N}\sum_{i=1}^N(f(x_i;w)-y_i)^2+ \frac{\lambda}{2}||w||^2$$

当第一项的经验风险较小，但是模型较为复杂（有多个非零参数）的时候，第二项的模型复杂度会比较大，起到了惩罚的作用给。
正则化的作用是选择经验风险和模型复杂度同时较小的模型。

## 交叉验证
样本充足的情况下，可以简单地将数据集分为训练集、验证集和测试集。
**训练集用来训练模型，验证集用于模型的选择，测试集用之于最终对学习方法的评估。**
样本不充足的时候，可以采用交叉验证的方法。

###1. 简单交叉验证
首先随机的将已给数据分成两部分，一部分作为训练集，另一部分作测试集，然后在各种条件下训练模型，从而得到不同的模型
###2. S折（s-fold）交叉验证
将已给数据随机等分成S个互不相交的子集，S-1个用作训练，1个用作测试集，将这一过程重复s次，选出测试误差最小的模型
###3. 留一交叉验证
是S折的特殊情况，仅在数据特别少的时候会用，这里S个子集每一个子集都只有一条数据

## 泛化误差
学习方法的繁华误差是指由该方法学习到的模型对未知数据的预测能力。
实际中往往用测试集进行量化，但是受限于测试集的质量。
泛化误差的定义：
如果学习到的模型是$\hat f$，那么用这个模型对未知数据预测的误差即为泛化误差
$$R_{exp}(\hat f)=E_P[L(Y, \hat f(X))] = \int_{x\times y}L(Y, \hat f(X))P(x,y)\mathrm dx\mathrm dy$$
泛化误差反映了学习方法的泛化能力，我们认为泛化误差小的模型更有效
**泛化误差就是模型的期望风险**

### 泛化误差上界
对于一组数据，假设空间是函数的有限集合$F=\{f_1,f_2,···,f_d\}$，d是集合中函数的个数。设$f$是从$F$中选取的函数，损失函数是0-1损失（二分类），那么关于f的期望风险和经验风险分别是：
$$R(f)=E[L(Y,f(X))]$$
$$\hat R(f)=\frac{1}{N}\sum_{i=1}^NL(Y_i,f(X_i))$$
经验风险最小化函数是:
$$f_N=\argmax_{f\in F}\hat R(F)$$

经验风险最小的参数不一定在所有未知数据上表现最好，我们应当更关注$f_N$的泛化能力
$$R(f_N)=E[L(Y,f_N(X))]$$


## 生成式模型和判别式模型
生成方法学习数据中的联合概率分布$P(X,Y)$，然后求出条件概率分布作为预测的模型。
$$P(Y|X) = \frac{P(X,Y)}{P(X)}$$
生成式模型表示了X和Y的生成关系。

判别方法直接用数据学习一个决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型。
判别方法只关注对于给定的输入$X$，应该预测什么样的输出$Y$

生成方法
1. 可以还原出数据中的联合概率分布$P(X,Y)$；
2. 生成式方法的学习收敛速度更快（为什么）；
3. 当存在隐变量时，仍可以用生成方法进行学习，而不能用判别方法。

判别方法
1. 直接学习条件概率或者决策函数，学习准确率更高。
2. 由于忽略了数据的联合分布，可以对数据进行各种程度上的首相、定义特征并使用特征，因此可以简化学习问题。

## 感知机
感知机是SVM的基础，原理是在数据中找到一个超平面，能够分割数据为两类，但是感知机没有一个找到最小平面的动力，也没有相关证明，效果不如svm

## F Score
$$F_\beta-Score=(1+\beta ^2)\cdot \frac {Precision\cdot Recall} {\beta^2\cdot Precision + Recall}$$